{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5599a29c-f982-4166-afa2-71b9189983a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'sys' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "import pyrallis\n",
    "import torch\n",
    "import time\n",
    "from PIL import Image\n",
    "from config import RunConfig\n",
    "from tqdm import tqdm\n",
    "# from pipeline_attend_and_excite import AttendAndExcitePipeline\n",
    "# from pipeline_attend_and_excite_syngen import AttendAndExciteSynGenPipeline\n",
    "import sys\n",
    "sys.path.append(' ')\n",
    "sys.path.append('..')\n",
    "sys.path.append('.')\n",
    "print(sys)\n",
    "from utils.ptp_utils import AttentionStore\n",
    "from utils import ptp_utils\n",
    "from utils import vis_utils\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
    "import warnings\n",
    "from diffusers import LCMScheduler\n",
    "from LCM_Dreamshaper_v7.lcm_pipeline import LatentConsistencyModelPipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abe45ab3-220c-49dd-9069-b46613d28832",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(config: RunConfig):\n",
    "    device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    scheduler = LCMScheduler.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\", subfolder=\"scheduler\")\n",
    "    safety_checker = StableDiffusionSafetyChecker.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"safety_checker\")\n",
    "    stable = LatentConsistencyModelPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\", scheduler=scheduler, safety_checker=safety_checker).to(device)\n",
    "    tokenizer = stable.tokenizer\n",
    "    print(F\"tokenizer: {type(tokenizer)}\")\n",
    "    # stable.scheduler = LCMScheduler.from_config(stable.scheduler.config)\n",
    "    #stable.scheduler.set_timesteps(num_inference_steps=config.n_inference_steps,original_inference_steps=50,device=device)\n",
    "    return stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae8ba26-a769-4d21-ac5a-ee250ce58c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "class RunConfig:\n",
    "    # Guiding text prompt\n",
    "    prompt: str = ''\n",
    "    # Whether to use Stable Diffusion v2.1\n",
    "    sd_2_1: bool = False\n",
    "    # Which token indices to alter with attend-and-excite\n",
    "    token_indices: List[int] = None\n",
    "    # Which random seeds to use when generating\n",
    "    seeds: List[int] = field(default_factory=lambda: [42, 54, 234, 324, 1])\n",
    "    # Path to save all outputs to\n",
    "    output_path: Path = Path('./outputs')\n",
    "    # Number of denoising steps\n",
    "    n_inference_steps: int = 4\n",
    "    # Text guidance scale\n",
    "    guidance_scale: float = 2.5\n",
    "    # Number of denoising steps to apply attend-and-excite\n",
    "    max_iter_to_alter: int = 25\n",
    "    # Resolution of UNet to compute attention maps over\n",
    "    attention_res: int = 24\n",
    "    # Whether to run standard SD or attend-and-excite\n",
    "    run_standard_sd: bool = False\n",
    "    # Dictionary defining the iterations and desired thresholds to apply iterative latent refinement in\n",
    "    thresholds: Dict[int, float] = field(default_factory=lambda: {0: 0.05, 10: 0.5, 20: 0.8})\n",
    "    # Scale factor for updating the denoised latent z_t\n",
    "    scale_factor: int = 20\n",
    "    # Start and end values used for scaling the scale factor - decays linearly with the denoising timestep\n",
    "    scale_range: tuple = field(default_factory=lambda: (1.0, 0.5))\n",
    "    # Whether to apply the Gaussian smoothing before computing the maximum attention value for each subject token\n",
    "    smooth_attentions: bool = True\n",
    "    # Standard deviation for the Gaussian smoothing\n",
    "    sigma: float = 0.5\n",
    "    # Kernel size for the Gaussian smoothing\n",
    "    kernel_size: int = 3\n",
    "    # Whether to save cross attention maps for the final results\n",
    "    save_cross_attention_maps: bool = True\n",
    "    # dvmp dataset\n",
    "    dataset_path: str = './datasets/phrases1.csv'\n",
    "    # model selection\n",
    "    model: str = 'LCM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2fd51da-6c23-4d15-ae49-93838ec225ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89140b25737842379bbbba22b24d05dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have passed a non-standard module StableDiffusionSafetyChecker(\n",
      "  (vision_model): CLIPVisionModel(\n",
      "    (vision_model): CLIPVisionTransformer(\n",
      "      (embeddings): CLIPVisionEmbeddings(\n",
      "        (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
      "        (position_embedding): Embedding(257, 1024)\n",
      "      )\n",
      "      (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder): CLIPEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (1): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (2): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (3): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (4): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (5): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (6): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (7): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (8): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (9): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (10): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (11): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (12): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (13): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (14): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (15): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (16): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (17): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (18): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (19): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (20): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (21): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (22): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "          (23): CLIPEncoderLayer(\n",
      "            (self_attn): CLIPAttention(\n",
      "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): CLIPMLP(\n",
      "              (activation_fn): QuickGELUActivation()\n",
      "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            )\n",
      "            (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
      "). We cannot verify whether it has the correct type\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer: <class 'transformers.models.clip.tokenization_clip.CLIPTokenizer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt idx:   0%|                                       | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt idx:   0%|                                       | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 32\u001b[0m\n\u001b[1;32m     25\u001b[0m controller \u001b[38;5;241m=\u001b[39m AttentionStore()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# image = run_on_prompt(prompt=config.prompt,\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#                       model=stable,\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#                       controller=controller,\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#                       token_indices=token_indices,\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m#                       seed=g,\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#                       config=config)\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mstable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlcm_origin_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpil\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages\n\u001b[1;32m     33\u001b[0m image \u001b[38;5;241m=\u001b[39m image[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     34\u001b[0m te \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/s1/2_GENAI_CONS/attend_and_excite_lcm/LCM_Dreamshaper_v7/lcm_pipeline.py:284\u001b[0m, in \u001b[0;36mLatentConsistencyModelPipeline.__call__\u001b[0;34m(self, prompt, height, width, guidance_scale, num_images_per_prompt, latents, num_inference_steps, lcm_origin_steps, prompt_embeds, output_type, return_dict, cross_attention_kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m prompt_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_prompt(\n\u001b[1;32m    275\u001b[0m     prompt,\n\u001b[1;32m    276\u001b[0m     device,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     prompt_embeds\u001b[38;5;241m=\u001b[39mprompt_embeds,\n\u001b[1;32m    280\u001b[0m )\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# 4. Prepare timesteps\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_timesteps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlcm_origin_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mtimesteps\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# 5. Prepare latent variable\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/ldm/lib/python3.8/site-packages/diffusers/schedulers/scheduling_lcm.py:462\u001b[0m, in \u001b[0;36mLCMScheduler.set_timesteps\u001b[0;34m(self, num_inference_steps, device, original_inference_steps, timesteps, strength)\u001b[0m\n\u001b[1;32m    459\u001b[0m     inference_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloor(inference_indices)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint64)\n\u001b[1;32m    460\u001b[0m     timesteps \u001b[38;5;241m=\u001b[39m lcm_origin_timesteps[inference_indices]\n\u001b[0;32m--> 462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimesteps \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_step_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device ordinal\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "config = RunConfig\n",
    "stable = load_model(config)\n",
    "count = 0\n",
    "time_total = 0\n",
    "METHOD = 'CFG_LCM'\n",
    "\n",
    "dataset = pd.read_csv('./datasets/phrases1.csv')\n",
    "dataset_name = config.dataset_path.split(\"/\")[-1].split('.')[0]\n",
    "ts = time.time()\n",
    "for i in tqdm(range(len(dataset)), desc=\"Prompt idx\"):\n",
    "    dataset_prompt_output_path = config.output_path / dataset_name / f\"{i:003}\"\n",
    "    dataset_prompt_output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "    config.prompt = dataset.iloc[i].prompt\n",
    "    token_indices = dataset.iloc[i].item_indices\n",
    "    for seed in [42,54]:\n",
    "        print(f\"Seed: {seed}\")\n",
    "        img_path = dataset_prompt_output_path / f'{METHOD}_{config.model}_{seed}.png'\n",
    "        if img_path.exists():\n",
    "            continue\n",
    "\n",
    "        ts = time.time()\n",
    "        g = torch.Generator('cuda').manual_seed(seed)\n",
    "        controller = AttentionStore()\n",
    "        # image = run_on_prompt(prompt=config.prompt,\n",
    "        #                       model=stable,\n",
    "        #                       controller=controller,\n",
    "        #                       token_indices=token_indices,\n",
    "        #                       seed=g,\n",
    "        #                       config=config)\n",
    "        image = stable(prompt=config.prompt, guidance_scale=2.5, num_inference_steps=2, lcm_origin_steps=50, output_type=\"pil\").images\n",
    "        image = image[0]\n",
    "        te = time.time()\n",
    "        image.save(img_path)\n",
    "        time_total += (te-ts)\n",
    "        count += 1\n",
    "        # except:\n",
    "        #     print('FAILED:',i, config.prompt)\n",
    "\n",
    "te = time.time()\n",
    "print(f\"*** Total time spent: {time_total:.4f} ***\")\n",
    "print(f\"*** For one image: {time_total/count:.4f}\")\n",
    "with open(f\"{config.output_path}/Time_{METHOD}_{config.model}_{dataset_name}.txt\", 'w') as f:\n",
    "    f.write(f\"{time_total/count:.4f}\") \n",
    "\n",
    "\n",
    "# else:\n",
    "#     token_indices = get_indices_to_alter(stable, config.prompt) if config.token_indices is None else config.token_indices\n",
    "#     images = []\n",
    "#     for seed in config.seeds:\n",
    "#         print(f\"Seed: {seed}\")\n",
    "#         g = torch.Generator('cuda').manual_seed(seed)\n",
    "#         controller = AttentionStore()\n",
    "#         image = run_on_prompt(prompt=config.prompt,\n",
    "#                               model=stable,\n",
    "#                               controller=controller,\n",
    "#                               token_indices=token_indices,\n",
    "#                               seed=g,\n",
    "#                               config=config)\n",
    "#         prompt_output_path = config.output_path / config.prompt\n",
    "#         prompt_output_path.mkdir(exist_ok=True, parents=True)\n",
    "#         image.save(prompt_output_path / f'{seed}.png')\n",
    "#         images.append(image)\n",
    "\n",
    "    # save a grid of results across all seeds\n",
    "    # joined_image = vis_utils.get_image_grid(images)\n",
    "    # joined_image.save(config.output_path / f'{config.prompt}.png')\n",
    "    \n",
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beeb1c40-e0b5-46f4-9118-1942426fb64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf110bc-875d-4826-a0fe-70719a2b4a01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "ldm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
